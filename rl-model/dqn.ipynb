{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class DotsAndBoxesEnv(gym.Env):\n",
    "    def __init__(self, grid_size=(5, 5)):  \n",
    "        super(DotsAndBoxesEnv, self).__init__()\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.state = self.create_all_lines(grid_size)  # Dict[str, bool] -> key, isClicked -> key format: h-0-1\n",
    "        self.done = False\n",
    "        self.current_player = 1  # Player 1 starts\n",
    "\n",
    "        # ✅ Action Space (total lines)\n",
    "        self.action_space = spaces.Discrete(len(self.state))\n",
    "\n",
    "        # ✅ Observation Space (binary representation of lines)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(len(self.state),), dtype=np.int8)\n",
    "\n",
    "    def create_all_lines(self, size) -> Dict[str, bool]:\n",
    "        lines = {}\n",
    "        for i in range(size[0] + 1):  \n",
    "            for j in range(size[1] + 1):\n",
    "                if j < size[1]:\n",
    "                    lines[f\"v-{i}-{j}\"] = False\n",
    "                if i < size[0]:\n",
    "                    lines[f\"h-{i}-{j}\"] = False\n",
    "        return lines\n",
    "\n",
    "    def step(self, action: int):\n",
    "        if self.done:\n",
    "            return self._get_observation(), 0, self.done, {}\n",
    "\n",
    "        all_lines = sorted(self.state.keys())\n",
    "        selected_line_key = all_lines[action]\n",
    "\n",
    "        if self.state[selected_line_key]:  # Line already clicked\n",
    "            return self._get_observation(), 0, self.done, {}\n",
    "\n",
    "        self.state[selected_line_key] = True  # Mark line as clicked\n",
    "\n",
    "        reward = self.evaluate_board(selected_line_key)\n",
    "        self.done = self.check_game_over()\n",
    "\n",
    "        # Switch player ONLY IF no box was completed\n",
    "        if reward == 0:\n",
    "            self.current_player = 1 if self.current_player == 2 else 2\n",
    "            reward = -0.1\n",
    "\n",
    "        return self._get_observation(), reward, self.done, {}\n",
    "\n",
    "    def evaluate_board(self, last_move: str):\n",
    "        \"\"\"\n",
    "        Check if the last move completed any box.\n",
    "        Reward = +1 if at least one box was completed, else 0.\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        parts = last_move.split(\"-\")\n",
    "        orientation, x, y = parts[0], int(parts[1]), int(parts[2])\n",
    "\n",
    "        # Possible boxes affected by the last move\n",
    "        affected_boxes = []\n",
    "\n",
    "        if orientation == \"h\":  # Horizontal line\n",
    "            if x > 0:  # Upper box\n",
    "                affected_boxes.append((x - 1, y))\n",
    "            if x < self.grid_size[0]:  # Lower box\n",
    "                affected_boxes.append((x, y))\n",
    "        elif orientation == \"v\":  # Vertical line\n",
    "            if y > 0:  # Left box\n",
    "                affected_boxes.append((x, y - 1))\n",
    "            if y < self.grid_size[1]:  # Right box\n",
    "                affected_boxes.append((x, y))\n",
    "\n",
    "        # Check if any affected box is now completed\n",
    "        for i, j in affected_boxes:\n",
    "            if self.is_box_completed(i, j):\n",
    "                reward = reward + 1  # A box was completed\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def is_box_completed(self, i, j):\n",
    "        \"\"\"Check if a box at (i, j) is completed by verifying its four edges.\"\"\"\n",
    "\n",
    "        top = f\"h-{i}-{j}\"\n",
    "        bottom = f\"h-{i+1}-{j}\"\n",
    "        left = f\"v-{i}-{j}\"\n",
    "        right = f\"v-{i}-{j+1}\"\n",
    "\n",
    "        # Ensure all keys exist in self.state before accessing\n",
    "        return (\n",
    "            self.state.get(top, False) and\n",
    "            self.state.get(bottom, False) and\n",
    "            self.state.get(left, False) and\n",
    "            self.state.get(right, False)\n",
    "        )\n",
    "\n",
    "\n",
    "    def check_game_over(self):\n",
    "        return all(self.state.values())\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.create_all_lines(self.grid_size)\n",
    "        self.done = False\n",
    "        self.current_player = 1  # Player 1 starts\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        return np.array(list(self.state.values()), dtype=np.int8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 4.0\n",
      "Episode 2: Total Reward = 1.9000000000000004\n",
      "Episode 3: Total Reward = -2.1\n",
      "Episode 4: Total Reward = -2.1\n",
      "Episode 5: Total Reward = -0.10000000000000009\n",
      "Episode 6: Total Reward = -2.0\n",
      "Episode 7: Total Reward = -2.0\n",
      "Episode 8: Total Reward = 4.0\n",
      "Episode 9: Total Reward = 6.0\n",
      "Episode 10: Total Reward = 6.0\n",
      "Episode 11: Total Reward = -6.1\n",
      "Episode 12: Total Reward = -8.1\n",
      "Episode 13: Total Reward = -1.9999999999999996\n",
      "Episode 14: Total Reward = 8.0\n",
      "Episode 15: Total Reward = 4.0\n",
      "Episode 16: Total Reward = 11.9\n",
      "Episode 17: Total Reward = -6.1\n",
      "Episode 18: Total Reward = -14.1\n",
      "Episode 19: Total Reward = -2.0\n",
      "Episode 20: Total Reward = 4.0\n",
      "Episode 21: Total Reward = 2.0\n",
      "Episode 22: Total Reward = 3.9000000000000004\n",
      "Episode 23: Total Reward = 1.9\n",
      "Episode 24: Total Reward = -0.09999999999999964\n",
      "Episode 25: Total Reward = -10.1\n",
      "Episode 26: Total Reward = 2.0000000000000004\n",
      "Episode 27: Total Reward = -8.1\n",
      "Episode 28: Total Reward = -0.1\n",
      "Episode 29: Total Reward = -8.0\n",
      "Episode 30: Total Reward = -10.0\n",
      "Episode 31: Total Reward = -0.09999999999999964\n",
      "Episode 32: Total Reward = 10.0\n",
      "Episode 33: Total Reward = 8.0\n",
      "Episode 34: Total Reward = 12.0\n",
      "Episode 35: Total Reward = -0.09999999999999964\n",
      "Episode 36: Total Reward = -4.0\n",
      "Episode 37: Total Reward = -14.0\n",
      "Episode 38: Total Reward = 4.0\n",
      "Episode 39: Total Reward = -2.0\n",
      "Episode 40: Total Reward = -4.1\n",
      "Episode 41: Total Reward = 2.0000000000000004\n",
      "Episode 42: Total Reward = 8.0\n",
      "Episode 43: Total Reward = 12.0\n",
      "Episode 44: Total Reward = -1.9999999999999996\n",
      "Episode 45: Total Reward = 10.0\n",
      "Episode 46: Total Reward = 4.0\n",
      "Episode 47: Total Reward = -1.9999999999999996\n",
      "Episode 48: Total Reward = 3.9000000000000004\n",
      "Episode 49: Total Reward = 6.0\n",
      "Episode 50: Total Reward = 4.0\n",
      "Episode 51: Total Reward = 4.0\n",
      "Episode 52: Total Reward = -8.0\n",
      "Episode 53: Total Reward = -6.0\n",
      "Episode 54: Total Reward = -10.0\n",
      "Episode 55: Total Reward = -1.9999999999999996\n",
      "Episode 56: Total Reward = -4.1\n",
      "Episode 57: Total Reward = 4.0\n",
      "Episode 58: Total Reward = -0.1\n",
      "Episode 59: Total Reward = 2.0\n",
      "Episode 60: Total Reward = 8.0\n",
      "Episode 61: Total Reward = -0.09999999999999998\n",
      "Episode 62: Total Reward = -4.0\n",
      "Episode 63: Total Reward = -8.1\n",
      "Episode 64: Total Reward = 4.0\n",
      "Episode 65: Total Reward = 0.0\n",
      "Episode 66: Total Reward = -10.0\n",
      "Episode 67: Total Reward = 4.0\n",
      "Episode 68: Total Reward = -10.1\n",
      "Episode 69: Total Reward = 3.608224830031759e-16\n",
      "Episode 70: Total Reward = 10.0\n",
      "Episode 71: Total Reward = 4.0\n",
      "Episode 72: Total Reward = 8.0\n",
      "Episode 73: Total Reward = -4.1\n",
      "Episode 74: Total Reward = 6.0\n",
      "Episode 75: Total Reward = -12.1\n",
      "Episode 76: Total Reward = 14.0\n",
      "Episode 77: Total Reward = 4.440892098500626e-16\n",
      "Episode 78: Total Reward = 12.0\n",
      "Episode 79: Total Reward = -4.1\n",
      "Episode 80: Total Reward = -6.1\n",
      "Episode 81: Total Reward = -4.0\n",
      "Episode 82: Total Reward = -6.0\n",
      "Episode 83: Total Reward = 2.0000000000000004\n",
      "Episode 84: Total Reward = -10.1\n",
      "Episode 85: Total Reward = -2.0\n",
      "Episode 86: Total Reward = -4.1\n",
      "Episode 87: Total Reward = 12.0\n",
      "Episode 88: Total Reward = -2.0\n",
      "Episode 89: Total Reward = 4.0\n",
      "Episode 90: Total Reward = 0.0\n",
      "Episode 91: Total Reward = -8.1\n",
      "Episode 92: Total Reward = -4.0\n",
      "Episode 93: Total Reward = -8.1\n",
      "Episode 94: Total Reward = 2.0\n",
      "Episode 95: Total Reward = -4.0\n",
      "Episode 96: Total Reward = 4.0\n",
      "Episode 97: Total Reward = -6.1\n",
      "Episode 98: Total Reward = 8.0\n",
      "Episode 99: Total Reward = -4.1\n",
      "Episode 100: Total Reward = 3.9000000000000004\n",
      "Episode 101: Total Reward = 10.0\n",
      "Episode 102: Total Reward = 4.0\n",
      "Episode 103: Total Reward = 16.0\n",
      "Episode 104: Total Reward = -6.1\n",
      "Episode 105: Total Reward = -2.0\n",
      "Episode 106: Total Reward = 9.9\n",
      "Episode 107: Total Reward = -8.1\n",
      "Episode 108: Total Reward = -2.1\n",
      "Episode 109: Total Reward = -2.1\n",
      "Episode 110: Total Reward = -6.1\n",
      "Episode 111: Total Reward = 5.9\n",
      "Episode 112: Total Reward = -2.1\n",
      "Episode 113: Total Reward = 13.9\n",
      "Episode 114: Total Reward = 10.0\n",
      "Episode 115: Total Reward = 3.9\n",
      "Episode 116: Total Reward = -0.09999999999999964\n",
      "Episode 117: Total Reward = 4.0\n",
      "Episode 118: Total Reward = -10.1\n",
      "Episode 119: Total Reward = -8.1\n",
      "Episode 120: Total Reward = 7.9\n",
      "Episode 121: Total Reward = -10.1\n",
      "Episode 122: Total Reward = 1.9\n",
      "Episode 123: Total Reward = 8.0\n",
      "Episode 124: Total Reward = 3.9000000000000004\n",
      "Episode 125: Total Reward = 1.9000000000000004\n",
      "Episode 126: Total Reward = 4.440892098500626e-16\n",
      "Episode 127: Total Reward = -8.1\n",
      "Episode 128: Total Reward = 8.0\n",
      "Episode 129: Total Reward = -0.09999999999999956\n",
      "Episode 130: Total Reward = -6.1\n",
      "Episode 131: Total Reward = -0.10000000000000009\n",
      "Episode 132: Total Reward = 5.9\n",
      "Episode 133: Total Reward = -2.1\n",
      "Episode 134: Total Reward = 6.0\n",
      "Episode 135: Total Reward = -4.1\n",
      "Episode 136: Total Reward = 3.9000000000000004\n",
      "Episode 137: Total Reward = 4.0\n",
      "Episode 138: Total Reward = -4.1\n",
      "Episode 139: Total Reward = 12.0\n",
      "Episode 140: Total Reward = 3.9000000000000004\n",
      "Episode 141: Total Reward = 9.9\n",
      "Episode 142: Total Reward = 7.9\n",
      "Episode 143: Total Reward = 2.0000000000000004\n",
      "Episode 144: Total Reward = -8.326672684688674e-17\n",
      "Episode 145: Total Reward = -4.1\n",
      "Episode 146: Total Reward = -10.1\n",
      "Episode 147: Total Reward = 12.0\n",
      "Episode 148: Total Reward = -4.1\n",
      "Episode 149: Total Reward = 8.0\n",
      "Episode 150: Total Reward = 7.9\n",
      "Episode 151: Total Reward = 0.0\n",
      "Episode 152: Total Reward = -6.1\n",
      "Episode 153: Total Reward = 1.9\n",
      "Episode 154: Total Reward = -14.1\n",
      "Episode 155: Total Reward = 11.9\n",
      "Episode 156: Total Reward = 1.9000000000000004\n",
      "Episode 157: Total Reward = 5.9\n",
      "Episode 158: Total Reward = -12.1\n",
      "Episode 159: Total Reward = -10.0\n",
      "Episode 160: Total Reward = -10.1\n",
      "Episode 161: Total Reward = -2.0\n",
      "Episode 162: Total Reward = -2.1\n",
      "Episode 163: Total Reward = -4.1\n",
      "Episode 164: Total Reward = -12.0\n",
      "Episode 165: Total Reward = 3.9\n",
      "Episode 166: Total Reward = 3.9000000000000004\n",
      "Episode 167: Total Reward = -12.0\n",
      "Episode 168: Total Reward = 8.0\n",
      "Episode 169: Total Reward = -0.09999999999999964\n",
      "Episode 170: Total Reward = 4.0\n",
      "Episode 171: Total Reward = 11.9\n",
      "Episode 172: Total Reward = -4.0\n",
      "Episode 173: Total Reward = -10.1\n",
      "Episode 174: Total Reward = 7.9\n",
      "Episode 175: Total Reward = 2.0\n",
      "Episode 176: Total Reward = -0.10000000000000009\n",
      "Episode 177: Total Reward = -0.09999999999999953\n",
      "Episode 178: Total Reward = -10.0\n",
      "Episode 179: Total Reward = 9.9\n",
      "Episode 180: Total Reward = 2.0\n",
      "Episode 181: Total Reward = -6.1\n",
      "Episode 182: Total Reward = -1.9999999999999996\n",
      "Episode 183: Total Reward = 0.0\n",
      "Episode 184: Total Reward = 5.9\n",
      "Episode 185: Total Reward = -10.1\n",
      "Episode 186: Total Reward = 0.0\n",
      "Episode 187: Total Reward = -1.9999999999999996\n",
      "Episode 188: Total Reward = 8.0\n",
      "Episode 189: Total Reward = -2.1\n",
      "Episode 190: Total Reward = 1.9\n",
      "Episode 191: Total Reward = -0.1\n",
      "Episode 192: Total Reward = -3.9999999999999996\n",
      "Episode 193: Total Reward = 6.0\n",
      "Episode 194: Total Reward = 4.0\n",
      "Episode 195: Total Reward = -10.1\n",
      "Episode 196: Total Reward = -14.1\n",
      "Episode 197: Total Reward = -8.326672684688674e-17\n",
      "Episode 198: Total Reward = 1.9\n",
      "Episode 199: Total Reward = -12.0\n",
      "Episode 200: Total Reward = -4.1\n",
      "Episode 201: Total Reward = 8.0\n",
      "Episode 202: Total Reward = 2.0\n",
      "Episode 203: Total Reward = -4.0\n",
      "Episode 204: Total Reward = -0.09999999999999964\n",
      "Episode 205: Total Reward = -4.1\n",
      "Episode 206: Total Reward = -2.1\n",
      "Episode 207: Total Reward = 2.0000000000000004\n",
      "Episode 208: Total Reward = -0.09999999999999964\n",
      "Episode 209: Total Reward = -8.1\n",
      "Episode 210: Total Reward = -1.9999999999999996\n",
      "Episode 211: Total Reward = 4.0\n",
      "Episode 212: Total Reward = -0.1\n",
      "Episode 213: Total Reward = 0.0\n",
      "Episode 214: Total Reward = -4.1\n",
      "Episode 215: Total Reward = -12.0\n",
      "Episode 216: Total Reward = -8.0\n",
      "Episode 217: Total Reward = 2.0000000000000004\n",
      "Episode 218: Total Reward = 3.608224830031759e-16\n",
      "Episode 219: Total Reward = -2.0\n",
      "Episode 220: Total Reward = 4.0\n",
      "Episode 221: Total Reward = 10.0\n",
      "Episode 222: Total Reward = 5.9\n",
      "Episode 223: Total Reward = -12.1\n",
      "Episode 224: Total Reward = 3.9000000000000004\n",
      "Episode 225: Total Reward = -8.1\n",
      "Episode 226: Total Reward = 2.0\n",
      "Episode 227: Total Reward = 13.9\n",
      "Episode 228: Total Reward = -6.1\n",
      "Episode 229: Total Reward = 12.0\n",
      "Episode 230: Total Reward = 0.0\n",
      "Episode 231: Total Reward = -10.0\n",
      "Episode 232: Total Reward = 1.9\n",
      "Episode 233: Total Reward = -8.1\n",
      "Episode 234: Total Reward = -10.0\n",
      "Episode 235: Total Reward = -2.1\n",
      "Episode 236: Total Reward = -1.9999999999999996\n",
      "Episode 237: Total Reward = -10.1\n",
      "Episode 238: Total Reward = 9.9\n",
      "Episode 239: Total Reward = -8.1\n",
      "Episode 240: Total Reward = 7.9\n",
      "Episode 241: Total Reward = 4.0\n",
      "Episode 242: Total Reward = 5.9\n",
      "Episode 243: Total Reward = -6.1\n",
      "Episode 244: Total Reward = -6.1\n",
      "Episode 245: Total Reward = -4.1\n",
      "Episode 246: Total Reward = -10.1\n",
      "Episode 247: Total Reward = 3.9\n",
      "Episode 248: Total Reward = -6.1\n",
      "Episode 249: Total Reward = 3.9\n",
      "Episode 250: Total Reward = -3.9999999999999996\n",
      "Episode 251: Total Reward = 14.0\n",
      "Episode 252: Total Reward = -8.1\n",
      "Episode 253: Total Reward = 6.0\n",
      "Episode 254: Total Reward = 0.0\n",
      "Episode 255: Total Reward = 4.0\n",
      "Episode 256: Total Reward = -2.0999999999999996\n",
      "Episode 257: Total Reward = -2.0999999999999996\n",
      "Episode 258: Total Reward = -0.10000000000000009\n",
      "Episode 259: Total Reward = 7.9\n",
      "Episode 260: Total Reward = -4.1\n",
      "Episode 261: Total Reward = -0.10000000000000009\n",
      "Episode 262: Total Reward = 9.9\n",
      "Episode 263: Total Reward = -2.1\n",
      "Episode 264: Total Reward = 4.0\n",
      "Episode 265: Total Reward = -12.0\n",
      "Episode 266: Total Reward = 4.0\n",
      "Episode 267: Total Reward = -6.0\n",
      "Episode 268: Total Reward = 0.0\n",
      "Episode 269: Total Reward = 4.718447854656915e-16\n",
      "Episode 270: Total Reward = -6.1\n",
      "Episode 271: Total Reward = 1.9000000000000004\n",
      "Episode 272: Total Reward = 8.0\n",
      "Episode 273: Total Reward = 3.9000000000000004\n",
      "Episode 274: Total Reward = 3.9\n",
      "Episode 275: Total Reward = 2.0\n",
      "Episode 276: Total Reward = -4.1\n",
      "Episode 277: Total Reward = 3.9\n",
      "Episode 278: Total Reward = -4.1\n",
      "Episode 279: Total Reward = -0.10000000000000009\n",
      "Episode 280: Total Reward = -4.1\n",
      "Episode 281: Total Reward = 0.0\n",
      "Episode 282: Total Reward = -4.1\n",
      "Episode 283: Total Reward = 2.0000000000000004\n",
      "Episode 284: Total Reward = -1.9999999999999996\n",
      "Episode 285: Total Reward = -14.0\n",
      "Episode 286: Total Reward = -0.09999999999999953\n",
      "Episode 287: Total Reward = -4.1\n",
      "Episode 288: Total Reward = 9.9\n",
      "Episode 289: Total Reward = 1.9\n",
      "Episode 290: Total Reward = 7.9\n",
      "Episode 291: Total Reward = -6.0\n",
      "Episode 292: Total Reward = 6.0\n",
      "Episode 293: Total Reward = -4.1\n",
      "Episode 294: Total Reward = 2.0\n",
      "Episode 295: Total Reward = -2.0\n",
      "Episode 296: Total Reward = 4.0\n",
      "Episode 297: Total Reward = 6.0\n",
      "Episode 298: Total Reward = -6.1\n",
      "Episode 299: Total Reward = 4.440892098500626e-16\n",
      "Episode 300: Total Reward = -0.10000000000000009\n",
      "Episode 301: Total Reward = 0.0\n",
      "Episode 302: Total Reward = 4.0\n",
      "Episode 303: Total Reward = 1.9\n",
      "Episode 304: Total Reward = 5.9\n",
      "Episode 305: Total Reward = -6.1\n",
      "Episode 306: Total Reward = -8.1\n",
      "Episode 307: Total Reward = 4.0\n",
      "Episode 308: Total Reward = 6.0\n",
      "Episode 309: Total Reward = 6.0\n",
      "Episode 310: Total Reward = -10.0\n",
      "Episode 311: Total Reward = 3.9000000000000004\n",
      "Episode 312: Total Reward = -4.1\n",
      "Episode 313: Total Reward = 9.9\n",
      "Episode 314: Total Reward = -2.0999999999999996\n",
      "Episode 315: Total Reward = 0.0\n",
      "Episode 316: Total Reward = -2.0\n",
      "Episode 317: Total Reward = -6.0\n",
      "Episode 318: Total Reward = -10.0\n",
      "Episode 319: Total Reward = 5.9\n",
      "Episode 320: Total Reward = -3.9999999999999996\n",
      "Episode 321: Total Reward = 6.0\n",
      "Episode 322: Total Reward = 4.0\n",
      "Episode 323: Total Reward = 4.440892098500626e-16\n",
      "Episode 324: Total Reward = 5.9\n",
      "Episode 325: Total Reward = -12.1\n",
      "Episode 326: Total Reward = 2.7755575615628914e-17\n",
      "Episode 327: Total Reward = -0.09999999999999964\n",
      "Episode 328: Total Reward = 12.0\n",
      "Episode 329: Total Reward = -4.1\n",
      "Episode 330: Total Reward = 2.0\n",
      "Episode 331: Total Reward = -2.1\n",
      "Episode 332: Total Reward = -4.1\n",
      "Episode 333: Total Reward = -6.1\n",
      "Episode 334: Total Reward = 7.9\n",
      "Episode 335: Total Reward = 12.0\n",
      "Episode 336: Total Reward = 3.9000000000000004\n",
      "Episode 337: Total Reward = -8.0\n",
      "Episode 338: Total Reward = -4.1\n",
      "Episode 339: Total Reward = 1.9000000000000004\n",
      "Episode 340: Total Reward = 8.0\n",
      "Episode 341: Total Reward = 3.9\n",
      "Episode 342: Total Reward = 10.0\n",
      "Episode 343: Total Reward = -2.1\n",
      "Episode 344: Total Reward = 16.0\n",
      "Episode 345: Total Reward = -8.0\n",
      "Episode 346: Total Reward = -10.1\n",
      "Episode 347: Total Reward = -12.1\n",
      "Episode 348: Total Reward = -10.0\n",
      "Episode 349: Total Reward = 10.0\n",
      "Episode 350: Total Reward = 2.0000000000000004\n",
      "Episode 351: Total Reward = -2.1\n",
      "Episode 352: Total Reward = 14.0\n",
      "Episode 353: Total Reward = -1.9999999999999996\n",
      "Episode 354: Total Reward = 2.0000000000000004\n",
      "Episode 355: Total Reward = -0.09999999999999964\n",
      "Episode 356: Total Reward = -8.0\n",
      "Episode 357: Total Reward = -8.0\n",
      "Episode 358: Total Reward = -4.1\n",
      "Episode 359: Total Reward = 11.9\n",
      "Episode 360: Total Reward = 6.0\n",
      "Episode 361: Total Reward = -6.0\n",
      "Episode 362: Total Reward = 8.0\n",
      "Episode 363: Total Reward = 4.0\n",
      "Episode 364: Total Reward = 4.0\n",
      "Episode 365: Total Reward = -4.1\n",
      "Episode 366: Total Reward = 1.9\n",
      "Episode 367: Total Reward = -2.0999999999999996\n",
      "Episode 368: Total Reward = 2.0\n",
      "Episode 369: Total Reward = -6.1\n",
      "Episode 370: Total Reward = -0.1\n",
      "Episode 371: Total Reward = -0.09999999999999964\n",
      "Episode 372: Total Reward = -6.1\n",
      "Episode 373: Total Reward = -6.1\n",
      "Episode 374: Total Reward = -0.10000000000000009\n",
      "Episode 375: Total Reward = 6.0\n",
      "Episode 376: Total Reward = 5.9\n",
      "Episode 377: Total Reward = -4.1\n",
      "Episode 378: Total Reward = 0.0\n",
      "Episode 379: Total Reward = 4.0\n",
      "Episode 380: Total Reward = -12.1\n",
      "Episode 381: Total Reward = -4.1\n",
      "Episode 382: Total Reward = -0.09999999999999964\n",
      "Episode 383: Total Reward = 3.9\n",
      "Episode 384: Total Reward = -2.1\n",
      "Episode 385: Total Reward = -8.1\n",
      "Episode 386: Total Reward = -0.10000000000000009\n",
      "Episode 387: Total Reward = 8.0\n",
      "Episode 388: Total Reward = -12.1\n",
      "Episode 389: Total Reward = 4.0\n",
      "Episode 390: Total Reward = 0.0\n",
      "Episode 391: Total Reward = -3.9999999999999996\n",
      "Episode 392: Total Reward = 1.9000000000000004\n",
      "Episode 393: Total Reward = -4.1\n",
      "Episode 394: Total Reward = 5.9\n",
      "Episode 395: Total Reward = -6.1\n",
      "Episode 396: Total Reward = -12.1\n",
      "Episode 397: Total Reward = 5.9\n",
      "Episode 398: Total Reward = -6.1\n",
      "Episode 399: Total Reward = 4.0\n",
      "Episode 400: Total Reward = -8.1\n",
      "Episode 401: Total Reward = -2.1\n",
      "Episode 402: Total Reward = 1.9000000000000004\n",
      "Episode 403: Total Reward = -2.0999999999999996\n",
      "Episode 404: Total Reward = 1.9\n",
      "Episode 405: Total Reward = 10.0\n",
      "Episode 406: Total Reward = 10.0\n",
      "Episode 407: Total Reward = -4.1\n",
      "Episode 408: Total Reward = 6.0\n",
      "Episode 409: Total Reward = -6.1\n",
      "Episode 410: Total Reward = -1.9999999999999996\n",
      "Episode 411: Total Reward = -0.09999999999999956\n",
      "Episode 412: Total Reward = 7.9\n",
      "Episode 413: Total Reward = 9.9\n",
      "Episode 414: Total Reward = -8.1\n",
      "Episode 415: Total Reward = -4.1\n",
      "Episode 416: Total Reward = 14.0\n",
      "Episode 417: Total Reward = -10.1\n",
      "Episode 418: Total Reward = -8.0\n",
      "Episode 419: Total Reward = -3.9999999999999996\n",
      "Episode 420: Total Reward = -6.1\n",
      "Episode 421: Total Reward = -1.9999999999999996\n",
      "Episode 422: Total Reward = 3.9000000000000004\n",
      "Episode 423: Total Reward = -12.0\n",
      "Episode 424: Total Reward = 3.9\n",
      "Episode 425: Total Reward = 10.0\n",
      "Episode 426: Total Reward = -6.0\n",
      "Episode 427: Total Reward = 6.0\n",
      "Episode 428: Total Reward = -12.1\n",
      "Episode 429: Total Reward = 7.9\n",
      "Episode 430: Total Reward = -4.1\n",
      "Episode 431: Total Reward = 6.0\n",
      "Episode 432: Total Reward = 8.0\n",
      "Episode 433: Total Reward = 9.9\n",
      "Episode 434: Total Reward = 6.0\n",
      "Episode 435: Total Reward = -8.1\n",
      "Episode 436: Total Reward = -6.0\n",
      "Episode 437: Total Reward = 1.9000000000000004\n",
      "Episode 438: Total Reward = -8.1\n",
      "Episode 439: Total Reward = 2.0\n",
      "Episode 440: Total Reward = 3.608224830031759e-16\n",
      "Episode 441: Total Reward = 6.0\n",
      "Episode 442: Total Reward = 1.9000000000000004\n",
      "Episode 443: Total Reward = 1.9000000000000004\n",
      "Episode 444: Total Reward = -4.0\n",
      "Episode 445: Total Reward = -6.0\n",
      "Episode 446: Total Reward = 4.0\n",
      "Episode 447: Total Reward = 7.9\n",
      "Episode 448: Total Reward = -2.0999999999999996\n",
      "Episode 449: Total Reward = -12.0\n",
      "Episode 450: Total Reward = -10.0\n",
      "Episode 451: Total Reward = -8.1\n",
      "Episode 452: Total Reward = 6.0\n",
      "Episode 453: Total Reward = 2.0000000000000004\n",
      "Episode 454: Total Reward = -8.0\n",
      "Episode 455: Total Reward = 7.9\n",
      "Episode 456: Total Reward = 3.9000000000000004\n",
      "Episode 457: Total Reward = 3.9\n",
      "Episode 458: Total Reward = 3.9\n",
      "Episode 459: Total Reward = 6.0\n",
      "Episode 460: Total Reward = 3.9000000000000004\n",
      "Episode 461: Total Reward = 10.0\n",
      "Episode 462: Total Reward = 4.0\n",
      "Episode 463: Total Reward = 6.0\n",
      "Episode 464: Total Reward = -2.1\n",
      "Episode 465: Total Reward = -2.1\n",
      "Episode 466: Total Reward = 10.0\n",
      "Episode 467: Total Reward = -6.0\n",
      "Episode 468: Total Reward = 2.0000000000000004\n",
      "Episode 469: Total Reward = -6.1\n",
      "Episode 470: Total Reward = 2.0000000000000004\n",
      "Episode 471: Total Reward = -2.1\n",
      "Episode 472: Total Reward = -6.0\n",
      "Episode 473: Total Reward = -6.1\n",
      "Episode 474: Total Reward = 3.9\n",
      "Episode 475: Total Reward = -12.0\n",
      "Episode 476: Total Reward = 11.9\n",
      "Episode 477: Total Reward = -6.1\n",
      "Episode 478: Total Reward = -3.9999999999999996\n",
      "Episode 479: Total Reward = 6.0\n",
      "Episode 480: Total Reward = 14.0\n",
      "Episode 481: Total Reward = -8.0\n",
      "Episode 482: Total Reward = -3.9999999999999996\n",
      "Episode 483: Total Reward = 4.0\n",
      "Episode 484: Total Reward = -6.1\n",
      "Episode 485: Total Reward = -2.0\n",
      "Episode 486: Total Reward = 4.0\n",
      "Episode 487: Total Reward = -10.1\n",
      "Episode 488: Total Reward = -10.1\n",
      "Episode 489: Total Reward = 3.608224830031759e-16\n",
      "Episode 490: Total Reward = -8.0\n",
      "Episode 491: Total Reward = 1.9000000000000004\n",
      "Episode 492: Total Reward = -0.10000000000000009\n",
      "Episode 493: Total Reward = 0.0\n",
      "Episode 494: Total Reward = 7.9\n",
      "Episode 495: Total Reward = 10.0\n",
      "Episode 496: Total Reward = -1.9999999999999996\n",
      "Episode 497: Total Reward = 16.0\n",
      "Episode 498: Total Reward = -6.1\n",
      "Episode 499: Total Reward = 6.0\n",
      "Episode 500: Total Reward = -10.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "import gym\n",
    "\n",
    "# Define the Q-network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 256)\n",
    "        self.fc4 = nn.Linear(256, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)  # Q-values for each action\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, lr=0.001, gamma=0.99, epsilon=1.0, epsilon_min=0.1, epsilon_decay=0.999, batch_size=32, memory_size=10000):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.memory = collections.deque(maxlen=memory_size)  # Experience Replay Buffer\n",
    "\n",
    "        self.model = DQN(input_dim=env.observation_space.shape[0], output_dim=env.action_space.n)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()  # Explore\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item()  # Exploit\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train_step(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return  # Skip training if not enough samples\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        # Compute Q-values\n",
    "        q_values = self.model(states).gather(1, actions).squeeze()\n",
    "\n",
    "        # Compute target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.model(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(q_values, target_q_values)\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train(self, num_episodes=1000):\n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                # If it's AI's turn (Player 1)\n",
    "                if self.env.current_player == 1:\n",
    "                    action = self.get_action(state)  # AI chooses action\n",
    "                    next_state, reward, done, _ = self.env.step(action)  # Take action\n",
    "                    self.store_experience(state, action, reward, next_state, done)\n",
    "                    self.train_step()  # Update model\n",
    "                    state = next_state\n",
    "                    total_reward += reward\n",
    "\n",
    "                # If it's opponent's turn (Player 2)\n",
    "                elif self.env.current_player == 2:\n",
    "                    # Here, the opponent plays randomly\n",
    "                    action = self.env.action_space.sample()  # Opponent chooses random action\n",
    "                    next_state, reward, done, _ = self.env.step(action)  # Take action\n",
    "                    state = next_state\n",
    "                    total_reward -= reward\n",
    "\n",
    "            # Decay epsilon for exploration-exploitation trade-off\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "            print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "    \n",
    "    def get_sorted_actions_with_scores(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state_tensor).squeeze().numpy()  # Get Q-values for each action\n",
    "\n",
    "        # Create a list of (action, q_value) pairs\n",
    "        actions_with_scores = [(i, q_value) for i, q_value in enumerate(q_values)]\n",
    "\n",
    "        # Sort the actions based on their Q-values in descending order (better actions first)\n",
    "        sorted_actions = sorted(actions_with_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return sorted_actions\n",
    "\n",
    "\n",
    "# Create Environment\n",
    "env = DotsAndBoxesEnv(grid_size=(5, 5))\n",
    "\n",
    "# Train the Agent\n",
    "agent = DQNAgent(env)\n",
    "agent.train(num_episodes=500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 11.200000000000001\n",
      "Episode 2: Total Reward = 11.1\n",
      "Episode 3: Total Reward = 11.199999999999998\n",
      "Episode 4: Total Reward = 11.200000000000001\n",
      "Episode 5: Total Reward = 11.2\n",
      "Episode 6: Total Reward = 11.1\n",
      "Episode 7: Total Reward = 11.4\n",
      "Episode 8: Total Reward = 11.3\n",
      "Episode 9: Total Reward = 10.999999999999996\n",
      "Episode 10: Total Reward = 11.2\n",
      "Average Reward over 10 episodes: 11.190000000000001\n"
     ]
    }
   ],
   "source": [
    "def evaluate_agent(agent, env, num_episodes=10):\n",
    "    total_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            # Get the action from the trained agent\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)  # Take action in the environment\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "    average_reward = np.mean(total_rewards)\n",
    "    print(f\"Average Reward over {num_episodes} episodes: {average_reward}\")\n",
    "\n",
    "# Evaluate the trained agent\n",
    "evaluate_agent(agent, env, num_episodes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "torch.save(agent.model.state_dict(), \"dots_and_boxes_dqn_model.pth\")\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = DQN(input_dim=env.observation_space.shape[0], output_dim=env.action_space.n)\n",
    "model.load_state_dict(torch.load(\"dots_and_boxes_dqn_model.pth\"))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 28, Score: 679778.75\n",
      "Action: 1, Score: 677874.5625\n",
      "Action: 38, Score: 677161.0\n",
      "Action: 15, Score: 676017.75\n",
      "Action: 17, Score: 675591.25\n",
      "Action: 11, Score: 673501.5625\n",
      "Action: 37, Score: 672632.1875\n",
      "Action: 48, Score: 671383.5625\n",
      "Action: 19, Score: 670306.75\n",
      "Action: 34, Score: 670180.5\n",
      "Action: 45, Score: 666825.4375\n",
      "Action: 18, Score: 666400.625\n",
      "Action: 0, Score: 665636.125\n",
      "Action: 8, Score: 659666.375\n",
      "Action: 23, Score: 659029.0\n",
      "Action: 33, Score: 657703.4375\n",
      "Action: 16, Score: 656669.125\n",
      "Action: 9, Score: 656541.8125\n",
      "Action: 46, Score: 655924.875\n",
      "Action: 53, Score: 655416.0625\n",
      "Action: 40, Score: 652610.625\n",
      "Action: 42, Score: 651752.9375\n",
      "Action: 21, Score: 651226.625\n",
      "Action: 22, Score: 651053.1875\n",
      "Action: 5, Score: 650585.1875\n",
      "Action: 4, Score: 650430.875\n",
      "Action: 6, Score: 649842.0\n",
      "Action: 43, Score: 648732.6875\n",
      "Action: 56, Score: 648468.4375\n",
      "Action: 24, Score: 647016.9375\n",
      "Action: 14, Score: 646586.4375\n",
      "Action: 10, Score: 645782.375\n",
      "Action: 29, Score: 645188.0\n",
      "Action: 47, Score: 644827.5\n",
      "Action: 31, Score: 644756.25\n",
      "Action: 26, Score: 643981.875\n",
      "Action: 41, Score: 643235.875\n",
      "Action: 44, Score: 643059.25\n",
      "Action: 30, Score: 643024.8125\n",
      "Action: 35, Score: 642861.25\n",
      "Action: 58, Score: 642615.1875\n",
      "Action: 49, Score: 642406.375\n",
      "Action: 54, Score: 640517.625\n",
      "Action: 59, Score: 639619.875\n",
      "Action: 57, Score: 638791.0\n",
      "Action: 3, Score: 637350.875\n",
      "Action: 55, Score: 636277.5625\n",
      "Action: 2, Score: 635544.8125\n",
      "Action: 51, Score: 632920.8125\n",
      "Action: 52, Score: 630656.375\n",
      "Action: 39, Score: 630463.0\n",
      "Action: 12, Score: 630135.5\n",
      "Action: 36, Score: 629793.125\n",
      "Action: 27, Score: 628050.75\n",
      "Action: 50, Score: 627526.6875\n",
      "Action: 13, Score: 626856.0625\n",
      "Action: 20, Score: 626521.9375\n",
      "Action: 32, Score: 624178.6875\n",
      "Action: 25, Score: 622771.25\n",
      "Action: 7, Score: 621589.375\n",
      "Action: 108, Score: 5820.43359375\n",
      "Action: 219, Score: 5591.2353515625\n",
      "Action: 122, Score: 5525.71875\n",
      "Action: 192, Score: 5152.6943359375\n",
      "Action: 65, Score: 5119.5205078125\n",
      "Action: 171, Score: 5038.84765625\n",
      "Action: 90, Score: 4861.484375\n",
      "Action: 202, Score: 4588.162109375\n",
      "Action: 139, Score: 4538.798828125\n",
      "Action: 84, Score: 4235.5859375\n",
      "Action: 201, Score: 4220.8583984375\n",
      "Action: 254, Score: 4029.05908203125\n",
      "Action: 67, Score: 3819.6689453125\n",
      "Action: 248, Score: 3451.349853515625\n",
      "Action: 223, Score: 3261.7783203125\n",
      "Action: 131, Score: 3242.8193359375\n",
      "Action: 123, Score: 2896.075927734375\n",
      "Action: 195, Score: 2815.70654296875\n",
      "Action: 125, Score: 2740.2333984375\n",
      "Action: 103, Score: 2678.955322265625\n",
      "Action: 214, Score: 2645.9951171875\n",
      "Action: 121, Score: 2581.214111328125\n",
      "Action: 180, Score: 2490.747802734375\n",
      "Action: 160, Score: 2478.322021484375\n",
      "Action: 166, Score: 2413.29638671875\n",
      "Action: 113, Score: 2412.54443359375\n",
      "Action: 137, Score: 2376.2666015625\n",
      "Action: 163, Score: 2336.19189453125\n",
      "Action: 73, Score: 2327.89306640625\n",
      "Action: 241, Score: 2271.92236328125\n",
      "Action: 179, Score: 2268.85205078125\n",
      "Action: 148, Score: 2225.116455078125\n",
      "Action: 162, Score: 2187.193115234375\n",
      "Action: 211, Score: 2186.86474609375\n",
      "Action: 106, Score: 2185.0625\n",
      "Action: 105, Score: 2184.28515625\n",
      "Action: 119, Score: 2165.587158203125\n",
      "Action: 231, Score: 2133.29345703125\n",
      "Action: 168, Score: 2132.889404296875\n",
      "Action: 99, Score: 2123.897705078125\n",
      "Action: 142, Score: 2055.287109375\n",
      "Action: 68, Score: 1977.291015625\n",
      "Action: 208, Score: 1960.6953125\n",
      "Action: 174, Score: 1955.9254150390625\n",
      "Action: 127, Score: 1924.365478515625\n",
      "Action: 235, Score: 1924.0765380859375\n",
      "Action: 117, Score: 1900.7130126953125\n",
      "Action: 93, Score: 1875.64453125\n",
      "Action: 147, Score: 1739.41845703125\n",
      "Action: 206, Score: 1717.3148193359375\n",
      "Action: 220, Score: 1699.121337890625\n",
      "Action: 190, Score: 1631.9696044921875\n",
      "Action: 246, Score: 1597.945068359375\n",
      "Action: 135, Score: 1534.11572265625\n",
      "Action: 170, Score: 1475.6044921875\n",
      "Action: 61, Score: 1412.623291015625\n",
      "Action: 101, Score: 1346.8905029296875\n",
      "Action: 104, Score: 1334.57080078125\n",
      "Action: 134, Score: 1324.33349609375\n",
      "Action: 221, Score: 1313.90234375\n",
      "Action: 102, Score: 1295.30859375\n",
      "Action: 167, Score: 1236.11328125\n",
      "Action: 87, Score: 1229.8829345703125\n",
      "Action: 146, Score: 1147.0458984375\n",
      "Action: 199, Score: 1130.37548828125\n",
      "Action: 85, Score: 1103.976318359375\n",
      "Action: 225, Score: 1080.0572509765625\n",
      "Action: 203, Score: 998.4483642578125\n",
      "Action: 76, Score: 983.3677368164062\n",
      "Action: 232, Score: 981.4609375\n",
      "Action: 126, Score: 922.6435546875\n",
      "Action: 151, Score: 901.7005004882812\n",
      "Action: 63, Score: 861.0957641601562\n",
      "Action: 253, Score: 855.0265502929688\n",
      "Action: 109, Score: 818.032958984375\n",
      "Action: 212, Score: 799.3143310546875\n",
      "Action: 240, Score: 726.4495849609375\n",
      "Action: 185, Score: 723.8218994140625\n",
      "Action: 120, Score: 722.8682861328125\n",
      "Action: 89, Score: 715.60107421875\n",
      "Action: 239, Score: 704.883056640625\n",
      "Action: 92, Score: 689.9654541015625\n",
      "Action: 216, Score: 618.920166015625\n",
      "Action: 155, Score: 508.96075439453125\n",
      "Action: 165, Score: 492.4424133300781\n",
      "Action: 96, Score: 485.61962890625\n",
      "Action: 187, Score: 465.14080810546875\n",
      "Action: 242, Score: 463.6854248046875\n",
      "Action: 79, Score: 389.62738037109375\n",
      "Action: 86, Score: 369.62408447265625\n",
      "Action: 255, Score: 369.4412841796875\n",
      "Action: 69, Score: 288.75714111328125\n",
      "Action: 66, Score: 280.318115234375\n",
      "Action: 95, Score: 257.1150207519531\n",
      "Action: 217, Score: 227.8990478515625\n",
      "Action: 189, Score: 203.75830078125\n",
      "Action: 236, Score: 199.44256591796875\n",
      "Action: 77, Score: 183.488525390625\n",
      "Action: 175, Score: 179.405517578125\n",
      "Action: 198, Score: 157.4031982421875\n",
      "Action: 182, Score: 117.000732421875\n",
      "Action: 193, Score: 73.2255859375\n",
      "Action: 218, Score: 38.2237548828125\n",
      "Action: 149, Score: 6.66571044921875\n",
      "Action: 158, Score: -14.6759033203125\n",
      "Action: 249, Score: -176.71112060546875\n",
      "Action: 164, Score: -236.24609375\n",
      "Action: 71, Score: -259.96002197265625\n",
      "Action: 129, Score: -265.59832763671875\n",
      "Action: 227, Score: -289.9190673828125\n",
      "Action: 124, Score: -292.6512756347656\n",
      "Action: 233, Score: -383.934814453125\n",
      "Action: 161, Score: -395.316650390625\n",
      "Action: 178, Score: -399.173828125\n",
      "Action: 94, Score: -474.6626281738281\n",
      "Action: 112, Score: -485.9755859375\n",
      "Action: 83, Score: -493.0323486328125\n",
      "Action: 176, Score: -525.1856079101562\n",
      "Action: 247, Score: -548.8601684570312\n",
      "Action: 88, Score: -639.338623046875\n",
      "Action: 115, Score: -694.764892578125\n",
      "Action: 209, Score: -733.9244995117188\n",
      "Action: 136, Score: -770.624755859375\n",
      "Action: 107, Score: -806.4271240234375\n",
      "Action: 222, Score: -814.8387451171875\n",
      "Action: 224, Score: -846.9286499023438\n",
      "Action: 80, Score: -848.0621337890625\n",
      "Action: 128, Score: -860.2299194335938\n",
      "Action: 130, Score: -861.2614135742188\n",
      "Action: 132, Score: -866.0699462890625\n",
      "Action: 152, Score: -934.170166015625\n",
      "Action: 60, Score: -965.175537109375\n",
      "Action: 82, Score: -1132.853759765625\n",
      "Action: 157, Score: -1206.2364501953125\n",
      "Action: 215, Score: -1209.9615478515625\n",
      "Action: 183, Score: -1227.400390625\n",
      "Action: 188, Score: -1315.614013671875\n",
      "Action: 143, Score: -1319.3524169921875\n",
      "Action: 194, Score: -1333.143310546875\n",
      "Action: 234, Score: -1350.9776611328125\n",
      "Action: 204, Score: -1376.82666015625\n",
      "Action: 138, Score: -1421.64697265625\n",
      "Action: 118, Score: -1466.17626953125\n",
      "Action: 78, Score: -1560.056396484375\n",
      "Action: 100, Score: -1561.913330078125\n",
      "Action: 97, Score: -1664.474609375\n",
      "Action: 184, Score: -1686.891845703125\n",
      "Action: 251, Score: -1779.7568359375\n",
      "Action: 75, Score: -1797.510498046875\n",
      "Action: 110, Score: -1816.9044189453125\n",
      "Action: 133, Score: -1830.398681640625\n",
      "Action: 200, Score: -1845.5986328125\n",
      "Action: 244, Score: -1854.6092529296875\n",
      "Action: 159, Score: -1900.4794921875\n",
      "Action: 252, Score: -1912.777587890625\n",
      "Action: 229, Score: -1972.4678955078125\n",
      "Action: 172, Score: -1977.77978515625\n",
      "Action: 173, Score: -1989.764892578125\n",
      "Action: 250, Score: -2074.5595703125\n",
      "Action: 140, Score: -2128.704833984375\n",
      "Action: 226, Score: -2151.035400390625\n",
      "Action: 197, Score: -2173.423828125\n",
      "Action: 230, Score: -2194.420654296875\n",
      "Action: 150, Score: -2197.994384765625\n",
      "Action: 81, Score: -2218.599609375\n",
      "Action: 111, Score: -2267.371337890625\n",
      "Action: 245, Score: -2298.0322265625\n",
      "Action: 238, Score: -2340.795654296875\n",
      "Action: 116, Score: -2359.46142578125\n",
      "Action: 177, Score: -2394.2509765625\n",
      "Action: 141, Score: -2630.552734375\n",
      "Action: 74, Score: -2765.388671875\n",
      "Action: 237, Score: -2772.93408203125\n",
      "Action: 213, Score: -3017.1787109375\n",
      "Action: 144, Score: -3084.109375\n",
      "Action: 205, Score: -3329.33935546875\n",
      "Action: 62, Score: -3384.9931640625\n",
      "Action: 145, Score: -3389.1630859375\n",
      "Action: 154, Score: -3511.754638671875\n",
      "Action: 98, Score: -3675.693359375\n",
      "Action: 91, Score: -3738.587890625\n",
      "Action: 228, Score: -3775.20361328125\n",
      "Action: 156, Score: -3809.1103515625\n",
      "Action: 243, Score: -3848.0751953125\n",
      "Action: 191, Score: -4030.310546875\n",
      "Action: 70, Score: -4117.5146484375\n",
      "Action: 186, Score: -4199.4345703125\n",
      "Action: 64, Score: -4442.380859375\n",
      "Action: 114, Score: -5053.51806640625\n",
      "Action: 181, Score: -5150.8603515625\n",
      "Action: 196, Score: -5275.6484375\n",
      "Action: 153, Score: -5687.63671875\n",
      "Action: 72, Score: -5760.10546875\n",
      "Action: 169, Score: -5809.5595703125\n",
      "Action: 210, Score: -5969.5810546875\n",
      "Action: 207, Score: -6607.56640625\n"
     ]
    }
   ],
   "source": [
    "# Assume you already have a trained agent\n",
    "state = env.reset()  # Get the initial state\n",
    "sorted_actions = agent.get_sorted_actions_with_scores(state)\n",
    "\n",
    "# Print the sorted actions with their scores\n",
    "for action, score in sorted_actions:\n",
    "    print(f\"Action: {action}, Score: {score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
