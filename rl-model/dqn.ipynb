{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class DotsAndBoxesEnv(gym.Env):\n",
    "    def __init__(self, grid_size=(5, 5)):  \n",
    "        super(DotsAndBoxesEnv, self).__init__()\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.state = self.create_all_lines(grid_size)  # Dict[str, bool] -> key, isClicked -> key format: h-0-1\n",
    "        self.done = False\n",
    "        self.current_player = 1  # Player 1 starts\n",
    "\n",
    "        # ✅ Action Space (total lines)\n",
    "        self.action_space = spaces.Discrete(len(self.state))\n",
    "\n",
    "        # ✅ Observation Space (binary representation of lines)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(len(self.state),), dtype=np.int8)\n",
    "\n",
    "    def create_all_lines(self, size) -> Dict[str, bool]:\n",
    "        lines = {}\n",
    "        for i in range(size[0] + 1):  \n",
    "            for j in range(size[1] + 1):\n",
    "                if j < size[1]:\n",
    "                    lines[f\"v-{i}-{j}\"] = False\n",
    "                if i < size[0]:\n",
    "                    lines[f\"h-{i}-{j}\"] = False\n",
    "        return lines\n",
    "\n",
    "    def step(self, action: int):\n",
    "        if self.done:\n",
    "            return self._get_observation(), 0, self.done, {}\n",
    "\n",
    "        all_lines = sorted(self.state.keys())\n",
    "        selected_line_key = all_lines[action]\n",
    "\n",
    "        if self.state[selected_line_key]:  # Line already clicked\n",
    "            return self._get_observation(), 0, self.done, {}\n",
    "\n",
    "        self.state[selected_line_key] = True  # Mark line as clicked\n",
    "\n",
    "        reward = self.evaluate_board(selected_line_key)\n",
    "        self.done = self.check_game_over()\n",
    "\n",
    "        # Switch player ONLY IF no box was completed\n",
    "        if reward == 0:\n",
    "            self.current_player = 1 if self.current_player == 2 else 2\n",
    "            reward = -0.1\n",
    "\n",
    "        return self._get_observation(), reward, self.done, {}\n",
    "\n",
    "    def evaluate_board(self, last_move: str):\n",
    "        \"\"\"\n",
    "        Check if the last move completed any box.\n",
    "        Reward = +1 if at least one box was completed, else 0.\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        parts = last_move.split(\"-\")\n",
    "        orientation, x, y = parts[0], int(parts[1]), int(parts[2])\n",
    "\n",
    "        # Possible boxes affected by the last move\n",
    "        affected_boxes = []\n",
    "\n",
    "        if orientation == \"h\":  # Horizontal line\n",
    "            if x > 0:  # Upper box\n",
    "                affected_boxes.append((x - 1, y))\n",
    "            if x < self.grid_size[0]:  # Lower box\n",
    "                affected_boxes.append((x, y))\n",
    "        elif orientation == \"v\":  # Vertical line\n",
    "            if y > 0:  # Left box\n",
    "                affected_boxes.append((x, y - 1))\n",
    "            if y < self.grid_size[1]:  # Right box\n",
    "                affected_boxes.append((x, y))\n",
    "\n",
    "        # Check if any affected box is now completed\n",
    "        for i, j in affected_boxes:\n",
    "            if self.is_box_completed(i, j):\n",
    "                reward = reward + 1  # A box was completed\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def is_box_completed(self, i, j):\n",
    "        \"\"\"Check if a box at (i, j) is completed by verifying its four edges.\"\"\"\n",
    "\n",
    "        top = f\"h-{i}-{j}\"\n",
    "        bottom = f\"h-{i+1}-{j}\"\n",
    "        left = f\"v-{i}-{j}\"\n",
    "        right = f\"v-{i}-{j+1}\"\n",
    "\n",
    "        # Ensure all keys exist in self.state before accessing\n",
    "        return (\n",
    "            self.state.get(top, False) and\n",
    "            self.state.get(bottom, False) and\n",
    "            self.state.get(left, False) and\n",
    "            self.state.get(right, False)\n",
    "        )\n",
    "\n",
    "\n",
    "    def check_game_over(self):\n",
    "        return all(self.state.values())\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.create_all_lines(self.grid_size)\n",
    "        self.done = False\n",
    "        self.current_player = 1  # Player 1 starts\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        return np.array(list(self.state.values()), dtype=np.int8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 11.9\n",
      "Episode 2: Total Reward = -6.0\n",
      "Episode 3: Total Reward = 3.9000000000000004\n",
      "Episode 4: Total Reward = 8.0\n",
      "Episode 5: Total Reward = 5.9\n",
      "Episode 6: Total Reward = -2.1\n",
      "Episode 7: Total Reward = 3.9000000000000004\n",
      "Episode 8: Total Reward = 0.0\n",
      "Episode 9: Total Reward = 2.0\n",
      "Episode 10: Total Reward = 5.9\n",
      "Episode 11: Total Reward = 3.9000000000000004\n",
      "Episode 12: Total Reward = -8.0\n",
      "Episode 13: Total Reward = -2.1\n",
      "Episode 14: Total Reward = 4.0\n",
      "Episode 15: Total Reward = 5.9\n",
      "Episode 16: Total Reward = -10.1\n",
      "Episode 17: Total Reward = 4.0\n",
      "Episode 18: Total Reward = -10.1\n",
      "Episode 19: Total Reward = -8.1\n",
      "Episode 20: Total Reward = -2.1\n",
      "Episode 21: Total Reward = -4.1\n",
      "Episode 22: Total Reward = -14.1\n",
      "Episode 23: Total Reward = -0.09999999999999964\n",
      "Episode 24: Total Reward = -8.1\n",
      "Episode 25: Total Reward = -2.0999999999999996\n",
      "Episode 26: Total Reward = -2.0999999999999996\n",
      "Episode 27: Total Reward = 2.0000000000000004\n",
      "Episode 28: Total Reward = 3.9000000000000004\n",
      "Episode 29: Total Reward = -14.1\n",
      "Episode 30: Total Reward = 3.3306690738754696e-16\n",
      "Episode 31: Total Reward = -1.9999999999999996\n",
      "Episode 32: Total Reward = 4.440892098500626e-16\n",
      "Episode 33: Total Reward = 2.0\n",
      "Episode 34: Total Reward = -0.10000000000000009\n",
      "Episode 35: Total Reward = -4.1\n",
      "Episode 36: Total Reward = -2.1\n",
      "Episode 37: Total Reward = -0.10000000000000009\n",
      "Episode 38: Total Reward = 8.0\n",
      "Episode 39: Total Reward = -10.1\n",
      "Episode 40: Total Reward = -2.0999999999999996\n",
      "Episode 41: Total Reward = 4.0\n",
      "Episode 42: Total Reward = -10.0\n",
      "Episode 43: Total Reward = -6.0\n",
      "Episode 44: Total Reward = -10.0\n",
      "Episode 45: Total Reward = -2.1\n",
      "Episode 46: Total Reward = -8.1\n",
      "Episode 47: Total Reward = -8.1\n",
      "Episode 48: Total Reward = -2.1\n",
      "Episode 49: Total Reward = -1.9999999999999996\n",
      "Episode 50: Total Reward = -2.0\n",
      "Episode 51: Total Reward = 1.9000000000000004\n",
      "Episode 52: Total Reward = 1.9000000000000004\n",
      "Episode 53: Total Reward = -8.1\n",
      "Episode 54: Total Reward = -6.0\n",
      "Episode 55: Total Reward = 11.9\n",
      "Episode 56: Total Reward = 4.0\n",
      "Episode 57: Total Reward = -0.10000000000000009\n",
      "Episode 58: Total Reward = 0.0\n",
      "Episode 59: Total Reward = 2.0000000000000004\n",
      "Episode 60: Total Reward = 2.0000000000000004\n",
      "Episode 61: Total Reward = 3.9\n",
      "Episode 62: Total Reward = 4.0\n",
      "Episode 63: Total Reward = 3.9\n",
      "Episode 64: Total Reward = -2.1\n",
      "Episode 65: Total Reward = -12.1\n",
      "Episode 66: Total Reward = 0.0\n",
      "Episode 67: Total Reward = 7.9\n",
      "Episode 68: Total Reward = -2.0\n",
      "Episode 69: Total Reward = 8.0\n",
      "Episode 70: Total Reward = 5.9\n",
      "Episode 71: Total Reward = 0.0\n",
      "Episode 72: Total Reward = 4.0\n",
      "Episode 73: Total Reward = 5.9\n",
      "Episode 74: Total Reward = 4.0\n",
      "Episode 75: Total Reward = 0.0\n",
      "Episode 76: Total Reward = -4.1\n",
      "Episode 77: Total Reward = -2.0999999999999996\n",
      "Episode 78: Total Reward = 1.9000000000000004\n",
      "Episode 79: Total Reward = -8.1\n",
      "Episode 80: Total Reward = 5.9\n",
      "Episode 81: Total Reward = 10.0\n",
      "Episode 82: Total Reward = -1.9999999999999996\n",
      "Episode 83: Total Reward = 3.9\n",
      "Episode 84: Total Reward = 7.9\n",
      "Episode 85: Total Reward = -4.1\n",
      "Episode 86: Total Reward = 7.9\n",
      "Episode 87: Total Reward = 4.0\n",
      "Episode 88: Total Reward = -6.1\n",
      "Episode 89: Total Reward = -8.0\n",
      "Episode 90: Total Reward = 3.9\n",
      "Episode 91: Total Reward = 7.9\n",
      "Episode 92: Total Reward = 3.9000000000000004\n",
      "Episode 93: Total Reward = -8.0\n",
      "Episode 94: Total Reward = -8.0\n",
      "Episode 95: Total Reward = 11.9\n",
      "Episode 96: Total Reward = 0.0\n",
      "Episode 97: Total Reward = 3.9\n",
      "Episode 98: Total Reward = -8.0\n",
      "Episode 99: Total Reward = 4.0\n",
      "Episode 100: Total Reward = -8.1\n",
      "Episode 101: Total Reward = -8.326672684688674e-17\n",
      "Episode 102: Total Reward = -1.9999999999999996\n",
      "Episode 103: Total Reward = 2.0\n",
      "Episode 104: Total Reward = -6.1\n",
      "Episode 105: Total Reward = 8.0\n",
      "Episode 106: Total Reward = -1.9999999999999996\n",
      "Episode 107: Total Reward = 4.0\n",
      "Episode 108: Total Reward = -2.0999999999999996\n",
      "Episode 109: Total Reward = -8.0\n",
      "Episode 110: Total Reward = 2.0000000000000004\n",
      "Episode 111: Total Reward = 7.9\n",
      "Episode 112: Total Reward = -8.1\n",
      "Episode 113: Total Reward = 16.0\n",
      "Episode 114: Total Reward = -1.9999999999999996\n",
      "Episode 115: Total Reward = 4.0\n",
      "Episode 116: Total Reward = 0.0\n",
      "Episode 117: Total Reward = -2.1\n",
      "Episode 118: Total Reward = -4.1\n",
      "Episode 119: Total Reward = -8.0\n",
      "Episode 120: Total Reward = 16.0\n",
      "Episode 121: Total Reward = 6.0\n",
      "Episode 122: Total Reward = 9.9\n",
      "Episode 123: Total Reward = -12.1\n",
      "Episode 124: Total Reward = 2.0\n",
      "Episode 125: Total Reward = 10.0\n",
      "Episode 126: Total Reward = -2.0\n",
      "Episode 127: Total Reward = -4.0\n",
      "Episode 128: Total Reward = -12.1\n",
      "Episode 129: Total Reward = -2.1\n",
      "Episode 130: Total Reward = 3.9000000000000004\n",
      "Episode 131: Total Reward = -3.9999999999999996\n",
      "Episode 132: Total Reward = -12.1\n",
      "Episode 133: Total Reward = 8.0\n",
      "Episode 134: Total Reward = -6.0\n",
      "Episode 135: Total Reward = -4.1\n",
      "Episode 136: Total Reward = 5.9\n",
      "Episode 137: Total Reward = -6.1\n",
      "Episode 138: Total Reward = -6.0\n",
      "Episode 139: Total Reward = 3.9000000000000004\n",
      "Episode 140: Total Reward = -10.0\n",
      "Episode 141: Total Reward = -2.0999999999999996\n",
      "Episode 142: Total Reward = 9.9\n",
      "Episode 143: Total Reward = 10.0\n",
      "Episode 144: Total Reward = 10.0\n",
      "Episode 145: Total Reward = -4.1\n",
      "Episode 146: Total Reward = -6.1\n",
      "Episode 147: Total Reward = 11.9\n",
      "Episode 148: Total Reward = -2.1\n",
      "Episode 149: Total Reward = 8.0\n",
      "Episode 150: Total Reward = 4.440892098500626e-16\n",
      "Episode 151: Total Reward = 6.0\n",
      "Episode 152: Total Reward = -8.1\n",
      "Episode 153: Total Reward = -10.1\n",
      "Episode 154: Total Reward = -6.1\n",
      "Episode 155: Total Reward = -4.1\n",
      "Episode 156: Total Reward = 2.0000000000000004\n",
      "Episode 157: Total Reward = -2.0\n",
      "Episode 158: Total Reward = -4.1\n",
      "Episode 159: Total Reward = 12.0\n",
      "Episode 160: Total Reward = 1.9\n",
      "Episode 161: Total Reward = 4.0\n",
      "Episode 162: Total Reward = -14.1\n",
      "Episode 163: Total Reward = -1.9999999999999996\n",
      "Episode 164: Total Reward = -0.1\n",
      "Episode 165: Total Reward = 4.0\n",
      "Episode 166: Total Reward = -0.09999999999999964\n",
      "Episode 167: Total Reward = 6.0\n",
      "Episode 168: Total Reward = 10.0\n",
      "Episode 169: Total Reward = -6.1\n",
      "Episode 170: Total Reward = -2.0999999999999996\n",
      "Episode 171: Total Reward = 2.0\n",
      "Episode 172: Total Reward = -6.1\n",
      "Episode 173: Total Reward = -10.0\n",
      "Episode 174: Total Reward = -1.9999999999999996\n",
      "Episode 175: Total Reward = -12.1\n",
      "Episode 176: Total Reward = 3.9000000000000004\n",
      "Episode 177: Total Reward = -8.0\n",
      "Episode 178: Total Reward = 10.0\n",
      "Episode 179: Total Reward = 6.0\n",
      "Episode 180: Total Reward = -8.1\n",
      "Episode 181: Total Reward = -2.1\n",
      "Episode 182: Total Reward = 3.9\n",
      "Episode 183: Total Reward = -12.0\n",
      "Episode 184: Total Reward = -2.1\n",
      "Episode 185: Total Reward = -4.1\n",
      "Episode 186: Total Reward = 6.0\n",
      "Episode 187: Total Reward = 4.0\n",
      "Episode 188: Total Reward = -6.1\n",
      "Episode 189: Total Reward = 5.9\n",
      "Episode 190: Total Reward = -6.1\n",
      "Episode 191: Total Reward = -8.1\n",
      "Episode 192: Total Reward = -4.1\n",
      "Episode 193: Total Reward = -6.1\n",
      "Episode 194: Total Reward = 4.0\n",
      "Episode 195: Total Reward = 3.608224830031759e-16\n",
      "Episode 196: Total Reward = 4.0\n",
      "Episode 197: Total Reward = 3.9000000000000004\n",
      "Episode 198: Total Reward = -6.1\n",
      "Episode 199: Total Reward = 11.9\n",
      "Episode 200: Total Reward = -10.1\n",
      "Episode 201: Total Reward = 2.0\n",
      "Episode 202: Total Reward = 6.0\n",
      "Episode 203: Total Reward = 4.0\n",
      "Episode 204: Total Reward = 3.608224830031759e-16\n",
      "Episode 205: Total Reward = 2.0\n",
      "Episode 206: Total Reward = -0.09999999999999964\n",
      "Episode 207: Total Reward = 4.0\n",
      "Episode 208: Total Reward = -2.1\n",
      "Episode 209: Total Reward = 0.0\n",
      "Episode 210: Total Reward = 5.9\n",
      "Episode 211: Total Reward = 3.608224830031759e-16\n",
      "Episode 212: Total Reward = 2.0000000000000004\n",
      "Episode 213: Total Reward = 8.0\n",
      "Episode 214: Total Reward = -4.1\n",
      "Episode 215: Total Reward = -4.1\n",
      "Episode 216: Total Reward = 3.9000000000000004\n",
      "Episode 217: Total Reward = -6.1\n",
      "Episode 218: Total Reward = -6.1\n",
      "Episode 219: Total Reward = -2.0\n",
      "Episode 220: Total Reward = 8.0\n",
      "Episode 221: Total Reward = 4.440892098500626e-16\n",
      "Episode 222: Total Reward = 4.0\n",
      "Episode 223: Total Reward = 7.9\n",
      "Episode 224: Total Reward = 6.0\n",
      "Episode 225: Total Reward = -0.09999999999999964\n",
      "Episode 226: Total Reward = -2.0999999999999996\n",
      "Episode 227: Total Reward = 4.0\n",
      "Episode 228: Total Reward = -0.09999999999999964\n",
      "Episode 229: Total Reward = -2.0\n",
      "Episode 230: Total Reward = 6.0\n",
      "Episode 231: Total Reward = -16.1\n",
      "Episode 232: Total Reward = -6.1\n",
      "Episode 233: Total Reward = -2.1\n",
      "Episode 234: Total Reward = 3.9000000000000004\n",
      "Episode 235: Total Reward = -2.0\n",
      "Episode 236: Total Reward = 0.0\n",
      "Episode 237: Total Reward = 6.0\n",
      "Episode 238: Total Reward = 12.0\n",
      "Episode 239: Total Reward = 3.9\n",
      "Episode 240: Total Reward = 4.0\n",
      "Episode 241: Total Reward = 8.0\n",
      "Episode 242: Total Reward = 4.440892098500626e-16\n",
      "Episode 243: Total Reward = -3.9999999999999996\n",
      "Episode 244: Total Reward = 4.0\n",
      "Episode 245: Total Reward = -10.1\n",
      "Episode 246: Total Reward = -4.0\n",
      "Episode 247: Total Reward = 14.0\n",
      "Episode 248: Total Reward = 2.0000000000000004\n",
      "Episode 249: Total Reward = 1.9000000000000004\n",
      "Episode 250: Total Reward = -6.1\n",
      "Episode 251: Total Reward = -3.9999999999999996\n",
      "Episode 252: Total Reward = 4.0\n",
      "Episode 253: Total Reward = 7.9\n",
      "Episode 254: Total Reward = -12.1\n",
      "Episode 255: Total Reward = -2.0\n",
      "Episode 256: Total Reward = -12.0\n",
      "Episode 257: Total Reward = -14.1\n",
      "Episode 258: Total Reward = -2.1\n",
      "Episode 259: Total Reward = 2.0000000000000004\n",
      "Episode 260: Total Reward = -4.1\n",
      "Episode 261: Total Reward = 2.0\n",
      "Episode 262: Total Reward = 11.9\n",
      "Episode 263: Total Reward = 4.0\n",
      "Episode 264: Total Reward = -4.1\n",
      "Episode 265: Total Reward = -8.0\n",
      "Episode 266: Total Reward = -4.1\n",
      "Episode 267: Total Reward = -2.0\n",
      "Episode 268: Total Reward = 7.9\n",
      "Episode 269: Total Reward = 2.0000000000000004\n",
      "Episode 270: Total Reward = -1.9999999999999996\n",
      "Episode 271: Total Reward = -0.09999999999999967\n",
      "Episode 272: Total Reward = -6.0\n",
      "Episode 273: Total Reward = 5.9\n",
      "Episode 274: Total Reward = 7.9\n",
      "Episode 275: Total Reward = -6.1\n",
      "Episode 276: Total Reward = 4.0\n",
      "Episode 277: Total Reward = 4.0\n",
      "Episode 278: Total Reward = -2.1\n",
      "Episode 279: Total Reward = -4.0\n",
      "Episode 280: Total Reward = -2.1\n",
      "Episode 281: Total Reward = -4.1\n",
      "Episode 282: Total Reward = -6.0\n",
      "Episode 283: Total Reward = 8.0\n",
      "Episode 284: Total Reward = 4.440892098500626e-16\n",
      "Episode 285: Total Reward = 6.0\n",
      "Episode 286: Total Reward = 5.9\n",
      "Episode 287: Total Reward = 1.9000000000000004\n",
      "Episode 288: Total Reward = 4.0\n",
      "Episode 289: Total Reward = -6.1\n",
      "Episode 290: Total Reward = 3.9\n",
      "Episode 291: Total Reward = -8.1\n",
      "Episode 292: Total Reward = 8.0\n",
      "Episode 293: Total Reward = -4.0\n",
      "Episode 294: Total Reward = -4.0\n",
      "Episode 295: Total Reward = -10.1\n",
      "Episode 296: Total Reward = -3.9999999999999996\n",
      "Episode 297: Total Reward = 14.0\n",
      "Episode 298: Total Reward = -4.1\n",
      "Episode 299: Total Reward = -0.09999999999999964\n",
      "Episode 300: Total Reward = 3.9\n",
      "Episode 301: Total Reward = 4.440892098500626e-16\n",
      "Episode 302: Total Reward = -4.1\n",
      "Episode 303: Total Reward = 0.0\n",
      "Episode 304: Total Reward = 1.9000000000000004\n",
      "Episode 305: Total Reward = -4.1\n",
      "Episode 306: Total Reward = 6.0\n",
      "Episode 307: Total Reward = 4.0\n",
      "Episode 308: Total Reward = -6.1\n",
      "Episode 309: Total Reward = -2.0999999999999996\n",
      "Episode 310: Total Reward = -2.1\n",
      "Episode 311: Total Reward = -3.9999999999999996\n",
      "Episode 312: Total Reward = -6.0\n",
      "Episode 313: Total Reward = 6.0\n",
      "Episode 314: Total Reward = -6.1\n",
      "Episode 315: Total Reward = 8.0\n",
      "Episode 316: Total Reward = -8.1\n",
      "Episode 317: Total Reward = -4.1\n",
      "Episode 318: Total Reward = -6.0\n",
      "Episode 319: Total Reward = 4.0\n",
      "Episode 320: Total Reward = 9.9\n",
      "Episode 321: Total Reward = -10.1\n",
      "Episode 322: Total Reward = 8.0\n",
      "Episode 323: Total Reward = -4.1\n",
      "Episode 324: Total Reward = -6.0\n",
      "Episode 325: Total Reward = 12.0\n",
      "Episode 326: Total Reward = 6.0\n",
      "Episode 327: Total Reward = -6.1\n",
      "Episode 328: Total Reward = 3.9000000000000004\n",
      "Episode 329: Total Reward = 12.0\n",
      "Episode 330: Total Reward = -4.1\n",
      "Episode 331: Total Reward = -10.1\n",
      "Episode 332: Total Reward = 10.0\n",
      "Episode 333: Total Reward = -6.0\n",
      "Episode 334: Total Reward = -0.09999999999999964\n",
      "Episode 335: Total Reward = -4.1\n",
      "Episode 336: Total Reward = -2.0\n",
      "Episode 337: Total Reward = -3.9999999999999996\n",
      "Episode 338: Total Reward = 4.0\n",
      "Episode 339: Total Reward = -6.0\n",
      "Episode 340: Total Reward = 10.0\n",
      "Episode 341: Total Reward = -4.1\n",
      "Episode 342: Total Reward = -6.1\n",
      "Episode 343: Total Reward = -6.0\n",
      "Episode 344: Total Reward = -10.1\n",
      "Episode 345: Total Reward = -6.1\n",
      "Episode 346: Total Reward = 8.0\n",
      "Episode 347: Total Reward = -8.326672684688674e-17\n",
      "Episode 348: Total Reward = -10.1\n",
      "Episode 349: Total Reward = 1.9\n",
      "Episode 350: Total Reward = 10.0\n",
      "Episode 351: Total Reward = 5.9\n",
      "Episode 352: Total Reward = 3.9000000000000004\n",
      "Episode 353: Total Reward = -6.1\n",
      "Episode 354: Total Reward = -8.326672684688674e-17\n",
      "Episode 355: Total Reward = 14.0\n",
      "Episode 356: Total Reward = -8.1\n",
      "Episode 357: Total Reward = 12.0\n",
      "Episode 358: Total Reward = 5.9\n",
      "Episode 359: Total Reward = -2.0\n",
      "Episode 360: Total Reward = -12.1\n",
      "Episode 361: Total Reward = -4.1\n",
      "Episode 362: Total Reward = 5.9\n",
      "Episode 363: Total Reward = -8.1\n",
      "Episode 364: Total Reward = -2.0\n",
      "Episode 365: Total Reward = -4.1\n",
      "Episode 366: Total Reward = -4.0\n",
      "Episode 367: Total Reward = -4.1\n",
      "Episode 368: Total Reward = 1.9000000000000004\n",
      "Episode 369: Total Reward = 6.0\n",
      "Episode 370: Total Reward = -1.9999999999999996\n",
      "Episode 371: Total Reward = 5.9\n",
      "Episode 372: Total Reward = 3.3306690738754696e-16\n",
      "Episode 373: Total Reward = -3.9999999999999996\n",
      "Episode 374: Total Reward = 3.9\n",
      "Episode 375: Total Reward = -6.1\n",
      "Episode 376: Total Reward = -2.0999999999999996\n",
      "Episode 377: Total Reward = -1.9999999999999996\n",
      "Episode 378: Total Reward = -8.1\n",
      "Episode 379: Total Reward = -0.10000000000000009\n",
      "Episode 380: Total Reward = 7.9\n",
      "Episode 381: Total Reward = 8.0\n",
      "Episode 382: Total Reward = -8.1\n",
      "Episode 383: Total Reward = -0.10000000000000009\n",
      "Episode 384: Total Reward = 3.608224830031759e-16\n",
      "Episode 385: Total Reward = 2.0\n",
      "Episode 386: Total Reward = -0.09999999999999964\n",
      "Episode 387: Total Reward = -6.1\n",
      "Episode 388: Total Reward = 8.0\n",
      "Episode 389: Total Reward = -6.0\n",
      "Episode 390: Total Reward = 7.9\n",
      "Episode 391: Total Reward = 7.9\n",
      "Episode 392: Total Reward = 5.9\n",
      "Episode 393: Total Reward = -2.0999999999999996\n",
      "Episode 394: Total Reward = -8.0\n",
      "Episode 395: Total Reward = 1.9\n",
      "Episode 396: Total Reward = -8.0\n",
      "Episode 397: Total Reward = 4.440892098500626e-16\n",
      "Episode 398: Total Reward = 0.0\n",
      "Episode 399: Total Reward = 4.0\n",
      "Episode 400: Total Reward = 4.0\n",
      "Episode 401: Total Reward = 5.9\n",
      "Episode 402: Total Reward = -6.1\n",
      "Episode 403: Total Reward = -4.0\n",
      "Episode 404: Total Reward = -0.10000000000000009\n",
      "Episode 405: Total Reward = -0.10000000000000009\n",
      "Episode 406: Total Reward = 2.0\n",
      "Episode 407: Total Reward = 10.0\n",
      "Episode 408: Total Reward = 6.0\n",
      "Episode 409: Total Reward = -0.10000000000000009\n",
      "Episode 410: Total Reward = 9.9\n",
      "Episode 411: Total Reward = 2.0\n",
      "Episode 412: Total Reward = -8.0\n",
      "Episode 413: Total Reward = 1.9\n",
      "Episode 414: Total Reward = -12.1\n",
      "Episode 415: Total Reward = -8.0\n",
      "Episode 416: Total Reward = 8.0\n",
      "Episode 417: Total Reward = 5.9\n",
      "Episode 418: Total Reward = -10.1\n",
      "Episode 419: Total Reward = -10.1\n",
      "Episode 420: Total Reward = 7.9\n",
      "Episode 421: Total Reward = 3.9000000000000004\n",
      "Episode 422: Total Reward = -8.0\n",
      "Episode 423: Total Reward = -2.1\n",
      "Episode 424: Total Reward = -10.1\n",
      "Episode 425: Total Reward = 3.9000000000000004\n",
      "Episode 426: Total Reward = 5.9\n",
      "Episode 427: Total Reward = -2.1\n",
      "Episode 428: Total Reward = -3.9999999999999996\n",
      "Episode 429: Total Reward = -2.0\n",
      "Episode 430: Total Reward = 6.0\n",
      "Episode 431: Total Reward = 7.9\n",
      "Episode 432: Total Reward = -1.9999999999999996\n",
      "Episode 433: Total Reward = 5.9\n",
      "Episode 434: Total Reward = -10.1\n",
      "Episode 435: Total Reward = 2.0\n",
      "Episode 436: Total Reward = 6.0\n",
      "Episode 437: Total Reward = -2.1\n",
      "Episode 438: Total Reward = -12.1\n",
      "Episode 439: Total Reward = -8.326672684688674e-17\n",
      "Episode 440: Total Reward = 12.0\n",
      "Episode 441: Total Reward = 8.0\n",
      "Episode 442: Total Reward = 3.9000000000000004\n",
      "Episode 443: Total Reward = -4.1\n",
      "Episode 444: Total Reward = 5.9\n",
      "Episode 445: Total Reward = 8.0\n",
      "Episode 446: Total Reward = -3.9999999999999996\n",
      "Episode 447: Total Reward = -2.0999999999999996\n",
      "Episode 448: Total Reward = 3.9000000000000004\n",
      "Episode 449: Total Reward = 2.0\n",
      "Episode 450: Total Reward = -6.1\n",
      "Episode 451: Total Reward = -8.0\n",
      "Episode 452: Total Reward = 3.9000000000000004\n",
      "Episode 453: Total Reward = 8.0\n",
      "Episode 454: Total Reward = 2.0\n",
      "Episode 455: Total Reward = 8.0\n",
      "Episode 456: Total Reward = -6.0\n",
      "Episode 457: Total Reward = 4.0\n",
      "Episode 458: Total Reward = 6.0\n",
      "Episode 459: Total Reward = 2.0000000000000004\n",
      "Episode 460: Total Reward = -3.9999999999999996\n",
      "Episode 461: Total Reward = 0.0\n",
      "Episode 462: Total Reward = 4.0\n",
      "Episode 463: Total Reward = -8.1\n",
      "Episode 464: Total Reward = 7.9\n",
      "Episode 465: Total Reward = -2.1\n",
      "Episode 466: Total Reward = -8.1\n",
      "Episode 467: Total Reward = 4.0\n",
      "Episode 468: Total Reward = 5.9\n",
      "Episode 469: Total Reward = -6.0\n",
      "Episode 470: Total Reward = 13.9\n",
      "Episode 471: Total Reward = -0.09999999999999998\n",
      "Episode 472: Total Reward = -6.1\n",
      "Episode 473: Total Reward = -10.0\n",
      "Episode 474: Total Reward = -2.0999999999999996\n",
      "Episode 475: Total Reward = -0.1\n",
      "Episode 476: Total Reward = 6.0\n",
      "Episode 477: Total Reward = -10.0\n",
      "Episode 478: Total Reward = -14.1\n",
      "Episode 479: Total Reward = 16.0\n",
      "Episode 480: Total Reward = 0.0\n",
      "Episode 481: Total Reward = 0.0\n",
      "Episode 482: Total Reward = 2.0\n",
      "Episode 483: Total Reward = -2.1\n",
      "Episode 484: Total Reward = -2.1\n",
      "Episode 485: Total Reward = 1.9000000000000004\n",
      "Episode 486: Total Reward = 14.0\n",
      "Episode 487: Total Reward = 6.0\n",
      "Episode 488: Total Reward = -4.0\n",
      "Episode 489: Total Reward = 8.0\n",
      "Episode 490: Total Reward = -6.0\n",
      "Episode 491: Total Reward = 8.0\n",
      "Episode 492: Total Reward = 1.9000000000000004\n",
      "Episode 493: Total Reward = 3.9\n",
      "Episode 494: Total Reward = -2.1\n",
      "Episode 495: Total Reward = -4.0\n",
      "Episode 496: Total Reward = 11.9\n",
      "Episode 497: Total Reward = -6.0\n",
      "Episode 498: Total Reward = -4.0\n",
      "Episode 499: Total Reward = -6.1\n",
      "Episode 500: Total Reward = -2.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "import gym\n",
    "\n",
    "# Define the Q-network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 256)\n",
    "        self.fc4 = nn.Linear(256, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)  # Q-values for each action\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, lr=0.001, gamma=0.99, epsilon=1.0, epsilon_min=0.1, epsilon_decay=0.999, batch_size=32, memory_size=10000):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.memory = collections.deque(maxlen=memory_size)  # Experience Replay Buffer\n",
    "\n",
    "        self.model = DQN(input_dim=env.observation_space.shape[0], output_dim=env.action_space.n)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()  # Explore\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item()  # Exploit\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train_step(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return  # Skip training if not enough samples\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        # Compute Q-values\n",
    "        q_values = self.model(states).gather(1, actions).squeeze()\n",
    "\n",
    "        # Compute target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.model(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(q_values, target_q_values)\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train(self, num_episodes=1000):\n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                # If it's AI's turn (Player 1)\n",
    "                if self.env.current_player == 1:\n",
    "                    action = self.get_action(state)  # AI chooses action\n",
    "                    next_state, reward, done, _ = self.env.step(action)  # Take action\n",
    "                    self.store_experience(state, action, reward, next_state, done)\n",
    "                    self.train_step()  # Update model\n",
    "                    state = next_state\n",
    "                    total_reward += reward\n",
    "\n",
    "                # If it's opponent's turn (Player 2)\n",
    "                elif self.env.current_player == 2:\n",
    "                    # Here, the opponent plays randomly\n",
    "                    action = self.env.action_space.sample()  # Opponent chooses random action\n",
    "                    next_state, reward, done, _ = self.env.step(action)  # Take action\n",
    "                    state = next_state\n",
    "                    total_reward -= reward\n",
    "\n",
    "            # Decay epsilon for exploration-exploitation trade-off\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "            print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "    \n",
    "    def get_sorted_actions_with_scores(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state_tensor).squeeze().numpy()  # Get Q-values for each action\n",
    "\n",
    "        # Create a list of (action, q_value) pairs\n",
    "        actions_with_scores = [(i, q_value) for i, q_value in enumerate(q_values)]\n",
    "\n",
    "        # Sort the actions based on their Q-values in descending order (better actions first)\n",
    "        sorted_actions = sorted(actions_with_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return sorted_actions\n",
    "\n",
    "\n",
    "# Create Environment\n",
    "env = DotsAndBoxesEnv(grid_size=(5, 5))\n",
    "\n",
    "# Train the Agent\n",
    "agent = DQNAgent(env)\n",
    "agent.train(num_episodes=500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 11.1\n",
      "Episode 2: Total Reward = 11.399999999999999\n",
      "Episode 3: Total Reward = 11.299999999999999\n",
      "Episode 4: Total Reward = 11.299999999999999\n",
      "Episode 5: Total Reward = 11.299999999999999\n",
      "Episode 6: Total Reward = 11.299999999999999\n",
      "Episode 7: Total Reward = 11.3\n",
      "Episode 8: Total Reward = 11.3\n",
      "Episode 9: Total Reward = 11.199999999999998\n",
      "Episode 10: Total Reward = 11.2\n",
      "Average Reward over 10 episodes: 11.27\n"
     ]
    }
   ],
   "source": [
    "def evaluate_agent(agent, env, num_episodes=10):\n",
    "    total_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            # Get the action from the trained agent\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)  # Take action in the environment\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "    average_reward = np.mean(total_rewards)\n",
    "    print(f\"Average Reward over {num_episodes} episodes: {average_reward}\")\n",
    "\n",
    "# Evaluate the trained agent\n",
    "evaluate_agent(agent, env, num_episodes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "torch.save(agent.model.state_dict(), \"dots_and_boxes_dqn_model.pth\")\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = DQN(input_dim=env.observation_space.shape[0], output_dim=env.action_space.n)\n",
    "model.load_state_dict(torch.load(\"dots_and_boxes_dqn_model.pth\"))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DQNAgent' object has no attribute 'get_sorted_actions_with_scores'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Assume you already have a trained agent\u001b[39;00m\n\u001b[1;32m      2\u001b[0m state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()  \u001b[38;5;66;03m# Get the initial state\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m sorted_actions \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sorted_actions_with_scores\u001b[49m(state)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Print the sorted actions with their scores\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action, score \u001b[38;5;129;01min\u001b[39;00m sorted_actions:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DQNAgent' object has no attribute 'get_sorted_actions_with_scores'"
     ]
    }
   ],
   "source": [
    "# Assume you already have a trained agent\n",
    "state = env.reset()  # Get the initial state\n",
    "sorted_actions = agent.get_sorted_actions_with_scores(state)\n",
    "\n",
    "# Print the sorted actions with their scores\n",
    "for action, score in sorted_actions:\n",
    "    print(f\"Action: {action}, Score: {score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
